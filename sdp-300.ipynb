{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":432296,"sourceType":"datasetVersion","datasetId":195056}],"dockerImageVersionId":30587,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nfrom tqdm.notebook import tqdm  #  tqdm is a Python library for adding progress bar\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import load_img, array_to_img\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import BinaryCrossentropy\n\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2023-11-15T16:01:24.541272Z","iopub.execute_input":"2023-11-15T16:01:24.541575Z","iopub.status.idle":"2023-11-15T16:01:36.404138Z","shell.execute_reply.started":"2023-11-15T16:01:24.541527Z","shell.execute_reply":"2023-11-15T16:01:36.403342Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BASE_DIR = '/kaggle/input/anime-faces/data/'","metadata":{"execution":{"iopub.status.busy":"2023-11-15T16:01:36.405592Z","iopub.execute_input":"2023-11-15T16:01:36.406110Z","iopub.status.idle":"2023-11-15T16:01:36.410621Z","shell.execute_reply.started":"2023-11-15T16:01:36.406084Z","shell.execute_reply":"2023-11-15T16:01:36.409549Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# load complete image paths to the list\nimage_paths = []\nfor image_name in os.listdir(BASE_DIR):\n    image_path = os.path.join(BASE_DIR, image_name)\n    image_paths.append(image_path)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T16:01:36.411919Z","iopub.execute_input":"2023-11-15T16:01:36.412185Z","iopub.status.idle":"2023-11-15T16:01:36.885303Z","shell.execute_reply.started":"2023-11-15T16:01:36.412162Z","shell.execute_reply":"2023-11-15T16:01:36.884440Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# remove unnecessary file\nimage_paths.remove('/kaggle/input/anime-faces/data/data')","metadata":{"execution":{"iopub.status.busy":"2023-11-15T16:01:36.887451Z","iopub.execute_input":"2023-11-15T16:01:36.887793Z","iopub.status.idle":"2023-11-15T16:01:36.892750Z","shell.execute_reply.started":"2023-11-15T16:01:36.887766Z","shell.execute_reply":"2023-11-15T16:01:36.891782Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Next we will visualize few images in the dataset\n\n# to display grid of images (7x7)\nplt.figure(figsize=(20, 20))\ntemp_images = image_paths[:49]\nindex = 1\n\nfor image_path in temp_images:\n    plt.subplot(7, 7, index)\n    # load the image\n    img = load_img(image_path)\n    # convert to numpy array\n    img = np.array(img)\n    # show the image\n    plt.imshow(img)\n    plt.axis('off')\n    # increment the index for next image\n    index += 1","metadata":{"execution":{"iopub.status.busy":"2023-11-15T16:01:36.893948Z","iopub.execute_input":"2023-11-15T16:01:36.894302Z","iopub.status.idle":"2023-11-15T16:01:40.140526Z","shell.execute_reply.started":"2023-11-15T16:01:36.894276Z","shell.execute_reply":"2023-11-15T16:01:40.139184Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Next we will preprocess the dataset\n\n# load the image and convert to numpy array\ntrain_images = [np.array(load_img(path)) for path in tqdm(image_paths)]\ntrain_images = np.array(train_images)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T16:01:40.141793Z","iopub.execute_input":"2023-11-15T16:01:40.142094Z","iopub.status.idle":"2023-11-15T16:03:32.414151Z","shell.execute_reply.started":"2023-11-15T16:01:40.142068Z","shell.execute_reply":"2023-11-15T16:03:32.413101Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_images[0].shape","metadata":{"execution":{"iopub.status.busy":"2023-11-15T16:03:32.415548Z","iopub.execute_input":"2023-11-15T16:03:32.415849Z","iopub.status.idle":"2023-11-15T16:03:32.421904Z","shell.execute_reply.started":"2023-11-15T16:03:32.415824Z","shell.execute_reply":"2023-11-15T16:03:32.421241Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Next we will reshape the numpy array\n\n# reshape the array\ntrain_images = train_images.reshape(train_images.shape[0], 64, 64, 3).astype('float32')","metadata":{"execution":{"iopub.status.busy":"2023-11-15T16:03:32.423479Z","iopub.execute_input":"2023-11-15T16:03:32.423826Z","iopub.status.idle":"2023-11-15T16:03:32.686779Z","shell.execute_reply.started":"2023-11-15T16:03:32.423795Z","shell.execute_reply":"2023-11-15T16:03:32.686000Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# normalize the images\ntrain_images = (train_images - 127.5) / 127.5","metadata":{"execution":{"iopub.status.busy":"2023-11-15T16:03:32.687830Z","iopub.execute_input":"2023-11-15T16:03:32.688090Z","iopub.status.idle":"2023-11-15T16:03:33.096919Z","shell.execute_reply.started":"2023-11-15T16:03:32.688066Z","shell.execute_reply":"2023-11-15T16:03:33.096120Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_images[0]","metadata":{"execution":{"iopub.status.busy":"2023-11-15T16:03:33.100851Z","iopub.execute_input":"2023-11-15T16:03:33.101606Z","iopub.status.idle":"2023-11-15T16:03:33.108801Z","shell.execute_reply.started":"2023-11-15T16:03:33.101568Z","shell.execute_reply":"2023-11-15T16:03:33.107894Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#First we will initialize the required values to the variables\n\n# Latent Space is an abstract, lower-dimensional representation of high-dimensional data,\n# often used in machine learning and data science to simplify complex data structures \n# and reveal hidden patterns.\n\n# latent dimension for random noise\n\n\nLATENT_DIM = 100\n# weight initializer\nWEIGHT_INIT = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n# no. of channels of the image\nCHANNELS = 3 # for gray scale, keep it as 1","metadata":{"execution":{"iopub.status.busy":"2023-11-15T16:03:33.110007Z","iopub.execute_input":"2023-11-15T16:03:33.110260Z","iopub.status.idle":"2023-11-15T16:03:33.118083Z","shell.execute_reply.started":"2023-11-15T16:03:33.110238Z","shell.execute_reply":"2023-11-15T16:03:33.117274Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generator Model will create new images similar to training data from random noise\n\nmodel = Sequential(name='generator')\n\n# 1d random noise\nmodel.add(layers.Dense(8 * 8 * 512, input_dim=LATENT_DIM))\nmodel.add(layers.ReLU())\n\n# convert 1d to 3d\nmodel.add(layers.Reshape((8, 8, 512)))\n\n# upsample to 16x16\nmodel.add(layers.Conv2DTranspose(256, (4, 4), strides=(2, 2), padding='same', kernel_initializer=WEIGHT_INIT))\nmodel.add(layers.ReLU())\n\n# upsample to 32x32\nmodel.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=WEIGHT_INIT))\nmodel.add(layers.ReLU())\n\n# upsample to 64x64\nmodel.add(layers.Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same', kernel_initializer=WEIGHT_INIT))\nmodel.add(layers.ReLU())\n\nmodel.add(layers.Conv2D(CHANNELS, (4, 4), padding='same', activation='tanh'))\n\ngenerator = model\ngenerator.summary()","metadata":{"execution":{"iopub.status.busy":"2023-11-15T16:03:33.119098Z","iopub.execute_input":"2023-11-15T16:03:33.119404Z","iopub.status.idle":"2023-11-15T16:03:36.137201Z","shell.execute_reply.started":"2023-11-15T16:03:33.119380Z","shell.execute_reply":"2023-11-15T16:03:36.136325Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Discriminator model will classify the image from the generator to check whether it real (or) fake images\n\nmodel = Sequential(name='discriminator')\ninput_shape = (64, 64, 3)\nalpha = 0.2\n\n# create conv layers\nmodel.add(layers.Conv2D(64, (4, 4), strides=(2, 2), padding='same', input_shape=input_shape))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.LeakyReLU(alpha=alpha))\n\nmodel.add(layers.Conv2D(128, (4, 4), strides=(2, 2), padding='same', input_shape=input_shape))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.LeakyReLU(alpha=alpha))\n\nmodel.add(layers.Conv2D(128, (4, 4), strides=(2, 2), padding='same', input_shape=input_shape))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.LeakyReLU(alpha=alpha))\n\nmodel.add(layers.Flatten())\nmodel.add(layers.Dropout(0.3))\n\n# output class\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\ndiscriminator = model\ndiscriminator.summary()","metadata":{"execution":{"iopub.status.busy":"2023-11-15T16:03:36.138342Z","iopub.execute_input":"2023-11-15T16:03:36.138652Z","iopub.status.idle":"2023-11-15T16:03:36.288916Z","shell.execute_reply.started":"2023-11-15T16:03:36.138625Z","shell.execute_reply":"2023-11-15T16:03:36.288068Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create DCGAN\n\nclass DCGAN(keras.Model):\n    def __init__(self, generator, discriminator, latent_dim):\n        super().__init__()\n        self.generator = generator\n        self.discriminator = discriminator\n        self.latent_dim = latent_dim\n        self.g_loss_metric = keras.metrics.Mean(name='g_loss')\n        self.d_loss_metric = keras.metrics.Mean(name='d_loss')\n        \n    @property\n    def metrics(self):\n        return [self.g_loss_metric, self.d_loss_metric]\n    \n    def compile(self, g_optimizer, d_optimizer, loss_fn):\n        super(DCGAN, self).compile()\n        self.g_optimizer = g_optimizer\n        self.d_optimizer = d_optimizer\n        self.loss_fn = loss_fn\n        \n    def train_step(self, real_images):\n        # get batch size from the data\n        batch_size = tf.shape(real_images)[0]\n        # generate random noise\n        random_noise = tf.random.normal(shape=(batch_size, self.latent_dim))\n        \n        # train the discriminator with real (1) and fake (0) images\n        with tf.GradientTape() as tape:\n            # compute loss on real images\n            pred_real = self.discriminator(real_images, training=True)\n            # generate real image labels\n            real_labels = tf.ones((batch_size, 1))\n            # label smoothing\n            real_labels += 0.05 * tf.random.uniform(tf.shape(real_labels))\n            d_loss_real = self.loss_fn(real_labels, pred_real)\n            \n            # compute loss on fake images\n            fake_images = self.generator(random_noise)\n            pred_fake = self.discriminator(fake_images, training=True)\n            # generate fake labels\n            fake_labels = tf.zeros((batch_size, 1))\n            d_loss_fake = self.loss_fn(fake_labels, pred_fake)\n            \n            # total discriminator loss\n            d_loss = (d_loss_real + d_loss_fake) / 2\n            \n        # compute discriminator gradients\n        gradients = tape.gradient(d_loss, self.discriminator.trainable_variables)\n        # update the gradients\n        self.d_optimizer.apply_gradients(zip(gradients, self.discriminator.trainable_variables))\n        \n        \n        # train the generator model\n        labels = tf.ones((batch_size, 1))\n        # generator want discriminator to think that fake images are real\n        with tf.GradientTape() as tape:\n            # generate fake images from generator\n            fake_images = self.generator(random_noise, training=True)\n            # classify images as real or fake\n            pred_fake = self.discriminator(fake_images, training=True)\n            # compute loss\n            g_loss = self.loss_fn(labels, pred_fake)\n            \n        # compute gradients\n        gradients = tape.gradient(g_loss, self.generator.trainable_variables)\n        # update the gradients\n        self.g_optimizer.apply_gradients(zip(gradients, self.generator.trainable_variables))\n        \n        # update states for both models\n        self.d_loss_metric.update_state(d_loss)\n        self.g_loss_metric.update_state(g_loss)\n        \n        return {'d_loss': self.d_loss_metric.result(), 'g_loss': self.g_loss_metric.result()}","metadata":{"execution":{"iopub.status.busy":"2023-11-15T16:03:36.290290Z","iopub.execute_input":"2023-11-15T16:03:36.290605Z","iopub.status.idle":"2023-11-15T16:03:36.305137Z","shell.execute_reply.started":"2023-11-15T16:03:36.290571Z","shell.execute_reply":"2023-11-15T16:03:36.304181Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Next we will plot some images for each epoch\n\nclass DCGANMonitor(keras.callbacks.Callback):\n    def __init__(self, num_imgs=25, latent_dim=100):\n        self.num_imgs = num_imgs\n        self.latent_dim = latent_dim\n        # create random noise for generating images\n        self.noise = tf.random.normal([25, latent_dim])\n\n    def on_epoch_end(self, epoch, logs=None):\n        # generate the image from noise\n        g_img = self.model.generator(self.noise)\n        # denormalize the image\n        g_img = (g_img * 127.5) + 127.5\n        g_img.numpy()\n        \n        fig = plt.figure(figsize=(8, 8))\n        for i in range(self.num_imgs):\n            plt.subplot(5, 5, i+1)\n            img = array_to_img(g_img[i])\n            plt.imshow(img)\n            plt.axis('off')\n        # plt.savefig('epoch_{:03d}.png'.format(epoch))\n        plt.show()\n        \n    def on_train_end(self, logs=None):\n        self.model.generator.save('generator.h5')","metadata":{"execution":{"iopub.status.busy":"2023-11-15T16:03:36.306173Z","iopub.execute_input":"2023-11-15T16:03:36.306448Z","iopub.status.idle":"2023-11-15T16:03:36.321541Z","shell.execute_reply.started":"2023-11-15T16:03:36.306424Z","shell.execute_reply":"2023-11-15T16:03:36.320748Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Now let us initialize the DCGAN model \n\ndcgan = DCGAN(generator=generator, discriminator=discriminator, latent_dim=LATENT_DIM)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T16:03:36.322878Z","iopub.execute_input":"2023-11-15T16:03:36.323736Z","iopub.status.idle":"2023-11-15T16:03:36.348907Z","shell.execute_reply.started":"2023-11-15T16:03:36.323703Z","shell.execute_reply":"2023-11-15T16:03:36.348001Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Next let us compile the DCGAN model\n\nD_LR = 0.0001 \nG_LR = 0.0003\ndcgan.compile(g_optimizer=Adam(learning_rate=G_LR, beta_1=0.5), d_optimizer=Adam(learning_rate=D_LR, beta_1=0.5), loss_fn=BinaryCrossentropy())","metadata":{"execution":{"iopub.status.busy":"2023-11-15T16:03:36.350358Z","iopub.execute_input":"2023-11-15T16:03:36.350717Z","iopub.status.idle":"2023-11-15T16:03:36.369114Z","shell.execute_reply.started":"2023-11-15T16:03:36.350685Z","shell.execute_reply":"2023-11-15T16:03:36.368421Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Next we will train the model\n\nN_EPOCHS = 50\ndcgan.fit(train_images, epochs=N_EPOCHS, callbacks=[DCGANMonitor()])","metadata":{"execution":{"iopub.status.busy":"2023-11-15T16:14:50.121615Z","iopub.execute_input":"2023-11-15T16:14:50.122299Z","iopub.status.idle":"2023-11-15T16:37:05.042699Z","shell.execute_reply.started":"2023-11-15T16:14:50.122262Z","shell.execute_reply":"2023-11-15T16:37:05.041699Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"noise = tf.random.normal([1, 100])\nfig = plt.figure(figsize=(3, 3))\n# generate the image from noise\ng_img = dcgan.generator(noise)\n# denormalize the image\ng_img = (g_img * 127.5) + 127.5\ng_img.numpy()\nimg = array_to_img(g_img[0])\nplt.imshow(img)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-15T16:37:38.259458Z","iopub.execute_input":"2023-11-15T16:37:38.259844Z","iopub.status.idle":"2023-11-15T16:37:38.328423Z","shell.execute_reply.started":"2023-11-15T16:37:38.259810Z","shell.execute_reply":"2023-11-15T16:37:38.326766Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle","metadata":{"execution":{"iopub.status.busy":"2023-11-15T16:37:44.451740Z","iopub.execute_input":"2023-11-15T16:37:44.452086Z","iopub.status.idle":"2023-11-15T16:37:44.456421Z","shell.execute_reply.started":"2023-11-15T16:37:44.452061Z","shell.execute_reply":"2023-11-15T16:37:44.455303Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pickle.dump(dcgan, open('/kaggle/working/model_ok.pkl', 'wb'))","metadata":{"execution":{"iopub.status.busy":"2023-11-15T16:38:52.436928Z","iopub.execute_input":"2023-11-15T16:38:52.437327Z","iopub.status.idle":"2023-11-15T16:38:52.627325Z","shell.execute_reply.started":"2023-11-15T16:38:52.437292Z","shell.execute_reply":"2023-11-15T16:38:52.626121Z"},"trusted":true},"outputs":[],"execution_count":null}]}